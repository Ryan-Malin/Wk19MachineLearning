{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random as rnd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before beginning, my prediction is that the linear regression will be a better fit\n",
    "# since most of the data's columns seem to be related. For example, missed payments,\n",
    "# number of bills going to collections, delinquencies, etc all generally indicate that\n",
    "# a higher value in those categories means a higher risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test and train data samples have already been split into two separate files\n",
    "# Pull in the csv files\n",
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'int_rate', 'installment', 'home_ownership', 'annual_inc',\n",
       "       'verification_status', 'pymnt_plan', 'dti', 'delinq_2yrs',\n",
       "       'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'total_acc',\n",
       "       'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
       "       'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',\n",
       "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
       "       'last_pymnt_amnt', 'collections_12_mths_ex_med', 'policy_code',\n",
       "       'application_type', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',\n",
       "       'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m',\n",
       "       'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',\n",
       "       'open_rv_24m', 'max_bal_bc', 'all_util', 'total_rev_hi_lim', 'inq_fi',\n",
       "       'total_cu_tl', 'inq_last_12m', 'acc_open_past_24mths', 'avg_cur_bal',\n",
       "       'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt',\n",
       "       'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op',\n",
       "       'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_recent_bc',\n",
       "       'mths_since_recent_inq', 'num_accts_ever_120_pd', 'num_actv_bc_tl',\n",
       "       'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl',\n",
       "       'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
       "       'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',\n",
       "       'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75',\n",
       "       'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim',\n",
       "       'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',\n",
       "       'hardship_flag', 'debt_settlement_flag', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the train_df columns\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify the number of columns\n",
    "len(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the train_df columns with the test_df columns and see how many are in common\n",
    "common = (train_df.columns == test_df.columns)\n",
    "len(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the X columns from the y column, isolating the target\n",
    "# Removed Unnamed and index because they are irrelevant\n",
    "X_train = train_df.drop('target', axis=1).copy()\n",
    "X_test = test_df.drop('target', axis=1).copy()\n",
    "y_train = train_df['target'].copy()\n",
    "y_test = test_df['target'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this identifies columns that do not match\n",
    "cols_to_add = set(X_train.columns) ^ set(X_test.columns)\n",
    "for col in cols_to_add:\n",
    "    X_test[col] = 0\n",
    "X_test = X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this switches the non-numberic columns into several numberic columns\n",
    "target_encoder = LabelEncoder().fit(y_train)\n",
    "y_train = target_encoder.transform(y_train)\n",
    "y_test = target_encoder.transform(y_test)\n",
    "y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5091450446618461"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression Model \n",
    "# this trains and scores the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier model and print the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This trains and scores the random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 150)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scales the data down significantly, typically between 0 and 1\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7598894087622289\n",
      "0.710919540229885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the scaled data and print the model score\n",
    "# The scores below show a good model to use. It is not over-fitted and shows reliability\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "print(lr.score(X_test_scaled, y_test))\n",
    "print(lr.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6509995746490855\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data and print the model score\n",
    "# this is an over-fit, since it's 100%\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "print(rf.score(X_test_scaled, y_test))\n",
    "print(rf.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.71592604e-02, 3.14331278e-02, 3.04747517e-02, 1.41740314e-02,\n",
       "       1.54956635e-02, 2.58530991e-03, 4.18670155e-03, 8.46111603e-03,\n",
       "       1.22234115e-03, 1.51668233e-02, 1.16199932e-02, 2.90506782e-02,\n",
       "       2.87885386e-02, 4.34494211e-02, 4.56721343e-02, 5.28972293e-02,\n",
       "       4.85726105e-02, 1.79203918e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.01468463e-01, 5.99583734e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.02184552e-03, 1.45226804e-02, 4.49778109e-03, 6.74043289e-03,\n",
       "       3.76408508e-03, 6.55625949e-03, 1.19530388e-02, 1.30482097e-02,\n",
       "       1.44377385e-02, 5.20809924e-03, 7.96856529e-03, 1.61847282e-02,\n",
       "       1.30606916e-02, 1.49773990e-02, 6.33873223e-03, 6.71147432e-03,\n",
       "       7.57513567e-03, 9.72818281e-03, 1.49380043e-02, 1.58019325e-02,\n",
       "       1.42213460e-02, 2.80931626e-04, 0.00000000e+00, 1.55251495e-02,\n",
       "       1.65339839e-02, 1.18820584e-02, 9.98721087e-03, 5.81183161e-03,\n",
       "       1.29155000e-02, 1.17402783e-02, 3.30649962e-03, 7.25521640e-03,\n",
       "       8.02147039e-03, 7.03166064e-03, 9.03589242e-03, 1.03049543e-02,\n",
       "       8.05923977e-03, 9.91319927e-03, 7.79434248e-03, 8.75168993e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.08256485e-03, 7.03972625e-03,\n",
       "       9.33081045e-03, 7.52299841e-03, 1.54106476e-03, 0.00000000e+00,\n",
       "       1.42154395e-02, 1.33979485e-02, 1.48941171e-02, 1.36543407e-02,\n",
       "       2.10307103e-04, 1.89289504e-03, 1.40648359e-03, 1.69635749e-03,\n",
       "       1.81410178e-03, 1.90328403e-03, 1.64831159e-03, 0.00000000e+00,\n",
       "       1.04241099e-03, 1.10709298e-03, 1.17398532e-03, 1.55764225e-03,\n",
       "       2.52562396e-03, 2.44832502e-03, 5.71985465e-05, 3.13271519e-05])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do the model scores compare to each other, and to the previous results on unscaled data? \n",
    "# The unscaled showed low scores, compared to the scaled data. I believe this is because\n",
    "# it allows various columns to be compared in the same range of quantities.\n",
    "\n",
    "#How does this compare to your prediction? Write down your results and thoughts.\n",
    "# My predicition seems to coincide with the results: the linear regression faired better.\n",
    "# I am not certain if this is because the columns seemed to have a similar theme or if it was due\n",
    "# to other reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
